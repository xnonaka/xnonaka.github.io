[
  {
    "objectID": "waterinsecurity.html",
    "href": "waterinsecurity.html",
    "title": "Water Insecurity in the US (2023)",
    "section": "",
    "text": "Water Insecurity in US Counties\nThis dataset contains the percentage of people lacking plumbing in US counties. Shown first is a plot of county population and the percent lacking plumbing. Almost all counties have less than 1% lacking plumbing, with higher percentages skewed towards the lower population side.\n\n\nCode\nwater_insecurity_2023 %&gt;% \n  ggplot(aes(x = log10(total_pop), \n             y = percent_lacking_plumbing)) +\n  geom_point() +\n  labs(x = \"log(Population)\", \n       y = \"Percent Lacking Plumbing\", \n       title = \"Percent Lacking Plumbing in US Counties\") +\n  theme_minimal()\n\n\n\n\n\nPeople Lacking Plumbing and Population in every US county\n\n\n\n\nTo analyze the data and simplify visualization, I took mean percentages across each state’s counties to display values for every state. I first grouped each state into U.S. Census regions.\n\n\nCode\nwest &lt;- c(\"Arizona\",\"Colorado\",\"Idaho\",\"Montana\",\"Nevada\",\"New Mexico\",\"Utah\",\"Wyoming\",\n               \"Alaska\",\"California\",\"Hawaii\",\"Oregon\",\"Washington\")\nnortheast &lt;- c(\"Connecticut\",\"Maine\",\"Massachusetts\",\"New Hampshire\",\"Rhode Island\",\"Vermont\",\"New Jersey\",\"New York\",\"Pennsylvania\")\nmidwest &lt;- c(\"Illinois\",\"Indiana\",\"Michigan\",\"Ohio\",\"Wisconsin\",\"Iowa\",\"Kansas\",\"Minnesota\",\"Missouri\",\"Nebraska\",\"North Dakota\",\"South Dakota\")\nsouth &lt;- c(\"Delaware\",\"Florida\",\"Georgia\",\"Maryland\",\"North Carolina\",\"South Carolina\",\"Virginia\",\"District of Columbia\",\"West Virginia\", \n               \"Alabama\",\"Kentucky\",\"Mississippi\",\"Tennessee\",\"Arkansas\",\"Louisiana\",\"Oklahoma\",\"Texas\",\"Puerto Rico\")\n\n\n\n\nCode\nwater_insecurity_2023 &lt;- mutate(water_insecurity_2023,\n    state = str_extract(name, \"(?&lt;=, ).*\"), \n    county = str_extract(name, \".*(?=,)\")\n  )\nwater_insecurity_2023 &lt;-  mutate(water_insecurity_2023,\n    region = case_when(\n      state %in% northeast ~ \"Northeast\",\n      state %in% midwest ~ \"Midwest\",\n      state %in% south ~ \"South\",\n      state %in% west ~ \"West\",\n      TRUE ~ NA_character_))\n\n\nI then used a jitter plot to display the mean state values for each region.\n\n\nCode\nstate_avg23 &lt;- water_insecurity_2023 %&gt;% \n  group_by(state, region) %&gt;% \n  summarize(avg_perc = mean(percent_lacking_plumbing, na.rm=TRUE)) \n\noutliers &lt;- state_avg23 %&gt;% \n  filter(avg_perc &gt; 0.4)\n\nggplot(state_avg23,\n  aes(x=region, \n      y=avg_perc, \n      color = region)) +\ngeom_jitter(width=.2, size=3, alpha=.7, show.legend = FALSE) +\ngeom_text_repel(data = outliers, aes(label = state), color = \"black\") +\nlabs(x=\"Region\",\n     y=\"Percentage Lacking Plumbing (State Mean)\",\n     title=\"Percentage Lacking Plumbing 2023 \"\n)\n\n\n\n\n\nMean State Percentage of People Lacking Plumbing by Region\n\n\n\n\nThe West region has three clear outliers, labeled in the plot. I was curious about the counties in those states, so I created a scatter plot with the counties in Alaska, Arizona, and New Mexico.\n\n\nCode\nstate_outliers &lt;- water_insecurity_2023 %&gt;% \n  filter(state == \"Alaska\" | state == \"Arizona\" | state == \"New Mexico\")\n\ncounty_outliers &lt;- state_outliers %&gt;% \n  filter(percent_lacking_plumbing &gt; .6)\n\nggplot(state_outliers, aes(x = log(total_pop), \n                   y = percent_lacking_plumbing, \n                   color = state)) +\n  geom_point(size = 3, \n             alpha = .7) +\n  geom_smooth(method = \"lm\", \n              color = \"black\") +\n  geom_text_repel(data = county_outliers, \n                  aes(label = county), \n                  color = \"black\") +\n  labs(x = \"log(Population)\", \n       y = \"Percent Lacking Plumbing\", \n       title = \"Water Insecurity and Population (AK, AZ, & NM)\") +\n  theme_minimal()\n\n\n\n\n\nPercent Lacking Plumbing in AK, AZ and NM counties\n\n\n\n\nBased on the plot, the counties with high, outlier values are lower population counties. Upon preliminary research of county-level census data, several of these counties had large Native American populations. I was curious to investigate the exact relationship between this demographic and water insecurity across all counties. I uploaded public census demographic data from the census website into a data frame, and joined it with the water insecurity data set.\n\n\nCode\ncensus_data &lt;- read.csv(\"/Users/xnonaka/Downloads/cc-est2024-alldata.csv\")\n\n\n\n\nCode\ncensus_data &lt;- census_data %&gt;% \n  select(STNAME, CTYNAME, TOT_POP, YEAR, IA_MALE, IA_FEMALE, AGEGRP) %&gt;% \n  mutate(IA_TOT = IA_MALE + IA_FEMALE, \n         IA_prop = (IA_MALE + IA_FEMALE)/TOT_POP) %&gt;% \n  filter(YEAR == 5, AGEGRP == 0) %&gt;% \n  rename(county = CTYNAME)\n\n\n\n\nCode\nmerged_data &lt;- census_data %&gt;% \n  left_join(water_insecurity_2023, by = \"county\")\n\n\n\n\nCode\nclean_data &lt;- merged_data %&gt;% \n  filter(!is.na(IA_prop), !is.na(percent_lacking_plumbing))\nplumbing_outliers &lt;- clean_data %&gt;% \n  filter(percent_lacking_plumbing &gt; .9)\n\n\n\n\nCode\nggplot(clean_data, \n       aes(x = IA_prop * 100, \n           y = percent_lacking_plumbing)) +\n  geom_point(alpha = 0.7, \n             color = \"steelblue2\") +\n  geom_smooth(method = \"lm\", \n              color = \"black\") +\n  geom_text_repel(data = plumbing_outliers, \n                  aes(label = county)) +\n  labs(x = \"Native American % of County Population\", \n       y = \"Percent Lacking Plumbing\", \n       title = \"Relationship Between Indigenous Population Share and Water Insecurity\") +\n  theme_minimal()\n\n\n\n\n\nPercent Lacking Plumbing and Native American Percent of County Population\n\n\n\n\nIn this plot, the regression line reveals a positive relationship between the proportions of Native American population and households lacking plumbing. The outliers in the West (Apache County, McKinley County) greatly influence the linear relationship. Propotion of Native Americans in county population is a heavily skewed stat, so I would be curious to investigate differences with other demographics or differences between only counties with high Native American populations.\nSources: Azadpour, Elmera and Nell, Cee, 2024, “Mapping water insecurity in R with tidycensus,” USGS, https://waterdata.usgs.gov/blog/acs-maps/\nU.S. Census Bureau, 2023, “Tenure by Plumbing Facilities,” American Community Survey, 1-Year Estimates Detailed Tables, Table B25049, accessed on Nov 27, 2024, https://data.census.gov/table?q=B25049&y=2023 .\nU.S. Census Bureau, 2023, “Total Population,” American Community Survey, 1-Year Estimates Detailed Tables, Table B01003, accessed on Nov 27, 2024, https://data.census.gov/table?q=B01003&y=2023 .\nU.S. Census Bureau, 2025, “County Population by Characteristics: 2020-2024”, Annual County Resident Population Estimates by Age, Sex, Race, and Hispanic Origin: April 1, 2020 to July 1, 2024, accessed on Dec 2, 2025, https://www.census.gov/data/tables/time-series/demo/popest/2020s-counties-detail.html\nhttps://github.com/rfordatascience/tidytuesday/tree/main/data/2025/2025-01-28 USGS - https://waterdata.usgs.gov/blog/acs-maps/"
  },
  {
    "objectID": "DSPres.html#data",
    "href": "DSPres.html#data",
    "title": "Water Insecurity in the US",
    "section": "Data",
    "text": "Data\nTidy Tuesday Water Insecurity Data - Observation at county level measured by percent of households lacking plumbing\n\nhead(water_insecurity_data)\n\n# A tibble: 6 × 4\n  name                            total_pop plumbing percent_lacking_plumbing\n  &lt;chr&gt;                               &lt;dbl&gt;    &lt;dbl&gt;                    &lt;dbl&gt;\n1 Baldwin County, Alabama            253507      271                   0.107 \n2 Houston County, Alabama            108462       30                   0.0277\n3 Los Angeles County, California    9663345     5248                   0.0543\n4 Santa Cruz County, California      261547      187                   0.0715\n5 Sonoma County, California          481812      308                   0.0639\n6 Contra Costa County, California   1155025      517                   0.0448"
  },
  {
    "objectID": "DSPres.html#regional-analysis",
    "href": "DSPres.html#regional-analysis",
    "title": "Water Insecurity in the US",
    "section": "Regional Analysis",
    "text": "Regional Analysis\nCreating regions (West example)\n\nwest &lt;- c(\"Arizona\",\"Colorado\",\"Idaho\",\"Montana\",\"Nevada\",\"New Mexico\",\"Utah\",\"Wyoming\",\n               \"Alaska\",\"California\",\"Hawaii\",\"Oregon\",\"Washington\")\n\nCreating separate “state” and “county” variables and assigning rows to region\n\nwater_insecurity_2023 &lt;- mutate(water_insecurity_2023,\n    state = str_extract(name, \"(?&lt;=, ).*\"), \n    county = str_extract(name, \".*(?=,)\")\n  )\nwater_insecurity_2023 &lt;-  mutate(water_insecurity_2023,\n    region = case_when(\n      state %in% northeast ~ \"Northeast\",\n      state %in% midwest ~ \"Midwest\",\n      state %in% south ~ \"South\",\n      state %in% west ~ \"West\",\n      ))"
  },
  {
    "objectID": "DSPres.html#regional-plot",
    "href": "DSPres.html#regional-plot",
    "title": "Water Insecurity in the US",
    "section": "Regional Plot",
    "text": "Regional Plot\n\n\nCode\n\n\nggplot(state_avg23,\n  aes(x=region, \n      y=avg_perc, \n      color = region)) +\ngeom_jitter(width = .2, \n            size=3.5, \n            alpha = .8, \n            show.legend = FALSE) +\ngeom_text_repel(data = outliers, \n                aes(label = state), \n                color = \"black\") +\nlabs(x=\"Region\",\n     y=\"Percentage Lacking Plumbing (State Mean)\",\n     title=\"Percentage Lacking Plumbing 2023 \"\n)"
  },
  {
    "objectID": "DSPres.html#examining-the-outliers",
    "href": "DSPres.html#examining-the-outliers",
    "title": "Water Insecurity in the US",
    "section": "Examining The Outliers",
    "text": "Examining The Outliers\n\n\nCode\n\n\nggplot(state_outliers, aes(x = log(total_pop), \n                   y = percent_lacking_plumbing, \n                   color = state)) +\n  geom_point(size = 3, \n             alpha = .7) +\n  geom_smooth(method = \"lm\", \n              color = \"black\") +\n  geom_text_repel(data = county_outliers, \n                  aes(label = county), \n                  color = \"black\") +\n  labs(x = \"log(Population)\", \n       y = \"Percent Lacking Plumbing\", \n       title = \"Water Insecurity and Population (AK, AZ, & NM)\") +\n  theme_minimal()"
  },
  {
    "objectID": "DSPres.html#examining-relationship-with-indigenous-populations",
    "href": "DSPres.html#examining-relationship-with-indigenous-populations",
    "title": "Water Insecurity in the US",
    "section": "Examining relationship with indigenous populations",
    "text": "Examining relationship with indigenous populations\nUploaded 2023 county-level data from census.gov\n\ncensus_data &lt;- census_data %&gt;% \n  select(STNAME, CTYNAME, TOT_POP, YEAR, IA_MALE, IA_FEMALE, AGEGRP) %&gt;% \n  mutate(IA_TOT = IA_MALE + IA_FEMALE, \n         IA_prop = (IA_MALE + IA_FEMALE)/TOT_POP) %&gt;% \n  filter(YEAR == 5, AGEGRP == 0) %&gt;% \n  rename(county = CTYNAME)\n\nJoined census data with water insecurity data by county name\n\nmerged_data &lt;- census_data %&gt;% \n  left_join(water_insecurity_2023, by = \"county\")"
  },
  {
    "objectID": "DSPres.html#plotting-the-relationship",
    "href": "DSPres.html#plotting-the-relationship",
    "title": "Water Insecurity in the US",
    "section": "Plotting the relationship",
    "text": "Plotting the relationship\n\n\nCode\n\n\nggplot(clean_data, \n       aes(x = IA_prop, \n           y = percent_lacking_plumbing)) +\n  geom_point(alpha = 0.7, \n             color = \"steelblue2\") +\n  geom_smooth(method = \"lm\", \n              color = \"black\") +\n  geom_text_repel(data = plumbing_outliers, \n                  label = plumbing_outliers$county) +\n  labs(x = \"Native American Prop of County Population\", \n       y = \"Percent Lacking Plumbing\", \n       title = \"Relationship Between Indigenous Population Share and Water Insecurity\") +\n  theme_minimal()"
  },
  {
    "objectID": "DSPres.html#sources",
    "href": "DSPres.html#sources",
    "title": "Water Insecurity in the US",
    "section": "Sources",
    "text": "Sources\nAzadpour, Elmera and Nell, Cee, 2024, “Mapping water insecurity in R with tidycensus,” USGS, https://waterdata.usgs.gov/blog/acs-maps/\nU.S. Census Bureau, 2023, “Tenure by Plumbing Facilities,” American Community Survey, 1-Year Estimates Detailed Tables, Table B25049, accessed on Nov 27, 2024, https://data.census.gov/table?q=B25049&y=2023 .\nU.S. Census Bureau, 2023, “Total Population,” American Community Survey, 1-Year Estimates Detailed Tables, Table B01003, accessed on Nov 27, 2024, https://data.census.gov/table?q=B01003&y=2023 .\nU.S. Census Bureau, 2025, “County Population by Characteristics: 2020-2024”, Annual County Resident Population Estimates by Age, Sex, Race, and Hispanic Origin: April 1, 2020 to July 1, 2024, accessed on Dec 2, 2025, https://www.census.gov/data/tables/time-series/demo/popest/2020s-counties-detail.html"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Penguin Permutation",
    "section": "",
    "text": "I investigate the difference in bill length between male and female penguins from the Palmer Penguins data set. Generally, male penguins are slightly larger than female penguins, but in some species the difference is minimal. I hope to determine if bill length is a good indicator of sex for the species in the data set (Adelie, Gentoo, and Chinstrap). The hypothesis I’m testing is that male penguins have longer bills than females. To do this, I use a permutation test to compare the observed bill length values for male and female penguins with a random distribution.\n\n\nCode\npenguin_data &lt;- penguins %&gt;% \n  filter(!is.na(sex))\npenguin_data %&gt;% \n  ggplot(aes(\n    x = sex, \n    y = bill_length_mm\n  )) +\n  geom_boxplot() +\n  labs(x = \"Sex\", \n       y = NULL, \n       title = \"Bill Length (mm)\")\n\n\n\n\n\n\n\n\n\nAverage bill length for males is slightly higher than females in the observed data.\n\n\nCode\npenguin_data %&gt;% \n  group_by(sex) %&gt;% \n  summarize(avg_bill = mean(bill_length_mm, na.rm = TRUE), \n            med_bill = median(bill_length_mm, na.rm = TRUE)) %&gt;% \n   summarize(avg_diff = diff(avg_bill), \n            med_diff = diff(med_bill))\n\n\n# A tibble: 1 × 2\n  avg_diff med_diff\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     3.76        4\n\n\n\n\nCode\npermutation &lt;- function(n, data){\n  data %&gt;% \n    select(sex, bill_length_mm) %&gt;%\n    filter(!is.na(bill_length_mm)) %&gt;% \n    mutate(bill_perm = sample(bill_length_mm, replace = FALSE)) %&gt;% \n    group_by(sex) %&gt;% \n    summarize(obs_avg = mean(bill_length_mm), \n              obs_med = median(bill_length_mm), \n              perm_avg = mean(bill_perm), \n              perm_med = median(bill_perm)) %&gt;%\n    summarize(obs_avg_diff = diff(obs_avg), \n              obs_med_diff = diff(obs_med), \n              perm_avg_diff = diff(perm_avg), \n              perm_med_diff = diff(perm_med), \n              rep = n)\n}\n\nmap(c(1:5), permutation, data = penguin_data) %&gt;% \n  list_rbind()\n\n\n# A tibble: 5 × 5\n  obs_avg_diff obs_med_diff perm_avg_diff perm_med_diff   rep\n         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1         3.76            4         0.765         2.40      1\n2         3.76            4         0.405         0.900     2\n3         3.76            4        -0.371        -1.35      3\n4         3.76            4         0.541         0.600     4\n5         3.76            4         0.383         0.300     5\n\n\nThe permutation function creates a “bill_perm” variable, which randomly selects a value from the bill_length_mm column and assigns it a new row. It does this for each row, so that the bill_perm column is randomly swapped bill length values. Then, it groups the rows by sex (male and female, NAs previously removed) and summarizes over the average and median of both the observed data (bill_length_mm) and the random data (bill_perm). Finally, it summarizes again using the diff function to show the difference between the bill_perm average and median between sexes.\n\n\nCode\nset.seed(131)\nperm_data &lt;- \n  map(c(1:500), permutation, data = penguin_data) %&gt;% \n  list_rbind()\n\n\nThe map function inputs a number vector into the permutation function. The function takes the argument n as the number of repetitions, so I map a vector 1:500 into the function to repeat it 500 times with the penguin data to generate 500 average bill length differences between male and female penguins under random chance.\n\n\nCode\nperm_data %&gt;% \n  ggplot(aes(\n    x = perm_avg_diff)) +\n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_avg_diff), color = \"red\") +\n  labs(x = \"Difference in mean\", \n       y = \"Count\", \n       title = \"Observed difference in mean vs null distribution\")\n\n\n\n\n\n\n\n\n\nThis histograms shows the distribution of the 500 differences between mean bill lengths from the randomly sorted bill length values. The red line is the observed difference in mean bill lengths at 3.76.\n\n\nCode\n\"A histogram showing the distribution of median differences under the null using the permutation function, with the observed difference shown in red.\" \n\n\n[1] \"A histogram showing the distribution of median differences under the null using the permutation function, with the observed difference shown in red.\"\n\n\nCode\nperm_data %&gt;% \n  ggplot(aes(\n    x = perm_med_diff)) +\n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_med_diff), color = \"red\") +\n  labs(\n    x = \"Difference in median\", \n    y = \"Count\", \n    title = \"Observed difference in median vs null distribution\"\n  )\n\n\n\n\n\n\n\n\n\nThis histogram shows the distribution of differences in medians between male and female from the random sort. The red line is the observed difference in medians of 4.\nAs can be seen in the graphs, p-values for both average difference and median difference are zero. Under the random distribution, the observed differences have a 0% chance to occur.\n\n\nCode\nperm_data %&gt;%  \n  summarize(pvalue_avg = mean(perm_avg_diff &gt; obs_avg_diff),\n            pvalue_med = mean(perm_med_diff &gt; obs_med_diff))\n\n\n# A tibble: 1 × 2\n  pvalue_avg pvalue_med\n       &lt;dbl&gt;      &lt;dbl&gt;\n1          0          0\n\n\nBased on these findings, the observed mean and median differences could not occur if bill length was distributed randomly (regardless of sex). The average male bill length is 3.76 mm greater than the female average, and the median is 4 mm greater. The permutation test shows that this observed difference is significant, and we can reject both null hypotheses for mean and median.\nData:\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Analyses of Text from Shakespeare’s Plays",
    "section": "",
    "text": "Code\nhamlet_words &lt;- hamlet %&gt;% \n  mutate(\n    light_words = str_count(dialogue, \"\\\\b(light|sun|day)\\\\b\"),\n    dark_words = str_count(dialogue, \"\\\\b(dark|black|night)\\\\b\")\n  ) %&gt;% \n  summarize(\n    light = sum(light_words),\n    dark = sum(dark_words)\n  )\n\nmacbeth_words &lt;- macbeth %&gt;% \n  mutate(\n    light_words = str_count(dialogue, \"\\\\b(light|sun|day)\\\\b\"),\n    dark_words = str_count(dialogue, \"\\\\b(dark|black|night)\\\\b\")\n  ) %&gt;% \n  summarize(\n    light = sum(light_words),\n    dark = sum(dark_words)\n  )\n\nromeo_juliet_words &lt;- romeo_juliet %&gt;% \n  mutate(\n    light_words = str_count(dialogue, \"\\\\b(light|sun|day)\\\\b\"),\n    dark_words = str_count(dialogue, \"\\\\b(dark|black|night)\\\\b\")\n  ) %&gt;% \n  summarize(\n    light = sum(light_words),\n    dark = sum(dark_words)\n  )\n\n\n\n\nCode\nlight_dark_words &lt;- data_frame(play = c(\"Hamlet\", \"Macbeth\", \"Romeo and Juliet\"), \n                               light = c(hamlet_words$light, macbeth_words$light, romeo_juliet_words$light), \n                               dark = c(hamlet_words$dark, macbeth_words$dark, romeo_juliet_words$dark))\nlight_dark_words &lt;- pivot_longer(light_dark_words, cols=c(light, dark), names_to=\"word type\", values_to=\"count\")\n\n\n\n\nCode\nggplot(light_dark_words, aes(\n  x=play, \n  y=count, \n  fill=`word type`\n)) +\n  geom_col() +\n  labs(x=\"Play\",\n       y=\"Word Count\",\n       title = \"Light and Dark Word Counts\")\n\n\n\n\n\n\n\n\n\nEach word type contains three words (dark has “dark”, “black”, and “night”, and light has “light”, “sun”, and “day”). These are very limited categories for such broad themes, but how often these word types occur in each play. Romeo and Juliet twice as many of these words as the other two plays (even though Hamlet has the most words total). This may reflect the kinds of imagery used in Romeo and Juliet, in which the light and dark types are split about evenly. Hamlet and Macbeth have slightly more dark words, which may illustrate the darker settings/vibes of these plays (even though they are all tragedies).\n\n\nCode\nwords_following_I &lt;- hamlet %&gt;%\n  mutate(\n    following_I = str_extract(dialogue,\"(?&lt;=\\\\bI\\\\s)\\\\w+\")\n  )\n  \n#words_following_I\n\n\n\n\nCode\nI_words &lt;- words_following_I %&gt;% \n  filter(!is.na(following_I)) %&gt;% \n  group_by(following_I) %&gt;% \n  summarize(count = n()) %&gt;% \n  arrange(desc(count))\n\n\nhead(I_words, 20)\n\n\n# A tibble: 20 × 2\n   following_I count\n   &lt;chr&gt;       &lt;int&gt;\n 1 have           48\n 2 am             43\n 3 will           43\n 4 do             31\n 5 know           19\n 6 pray           17\n 7 would          14\n 8 shall          12\n 9 had            11\n10 did            10\n11 must           10\n12 think          10\n13 cannot          8\n14 could           7\n15 saw             7\n16 see             7\n17 hope            5\n18 mean            5\n19 was             5\n20 can             4\n\n\nThis table shows the most common words following “I” in Hamlet. My goal was to identify the most common verbs from the play, and see if any uncommon or unexpected ones occur often. Shakespeare obviously used many words that are not widely used or understood anymore, but all of the most common verbs were very standard to today’s English. One drawback from my code is I was only able to extract the words following the first “I” in a single line (so if I was used twice, it didn’t extract the next word to add to the count).\n\n\nCode\nopposite_words_romeo_juliet &lt;- romeo_juliet %&gt;% \n  mutate(\n    w_love = str_detect(dialogue, \"\\\\blove\\\\b\"),\n    w_hate = str_detect(dialogue, \"\\\\bhate\\\\b\"),\n    w_life = str_detect(dialogue, \"\\\\blife\\\\b\"),\n    w_death = str_detect(dialogue, \"\\\\bdeath\\\\b\"),\n    w_heaven = str_detect(dialogue, \"\\\\bheaven\\\\b\"), \n    w_hell = str_detect(dialogue, \"\\\\bhell\\\\b\")\n  ) \nsum_words_romeo_juliet &lt;-opposite_words_romeo_juliet %&gt;% \n  summarize(\n    love = sum(w_love), \n    hate = sum(w_hate), \n    life = sum(w_life), \n    death = sum(w_death), \n    heaven = sum(w_heaven),\n    hell = sum(w_hell)\n  )\n\nopposite_words_hamlet &lt;- hamlet %&gt;% \n  mutate(\n    w_love = str_detect(dialogue, \"\\\\blove\\\\b\"),\n    w_hate = str_detect(dialogue, \"\\\\bhate\\\\b\"),\n    w_life = str_detect(dialogue, \"\\\\blife\\\\b\"),\n    w_death = str_detect(dialogue, \"\\\\bdeath\\\\b\"),\n    w_heaven = str_detect(dialogue, \"\\\\bheaven\\\\b\"), \n    w_hell = str_detect(dialogue, \"\\\\bhell\\\\b\")\n  ) \nsum_words_hamlet &lt;-opposite_words_hamlet %&gt;% \n  summarize(\n    love = sum(w_love), \n    hate = sum(w_hate), \n    life = sum(w_life), \n    death = sum(w_death), \n    heaven = sum(w_heaven),\n    hell = sum(w_hell)\n  )\n\nopposite_words_macbeth &lt;- macbeth %&gt;% \n  mutate(\n    w_love = str_detect(dialogue, \"\\\\blove\\\\b\"),\n    w_hate = str_detect(dialogue, \"\\\\bhate\\\\b\"),\n    w_life = str_detect(dialogue, \"\\\\blife\\\\b\"),\n    w_death = str_detect(dialogue, \"\\\\bdeath\\\\b\"),\n    w_heaven = str_detect(dialogue, \"\\\\bheaven\\\\b\"), \n    w_hell = str_detect(dialogue, \"\\\\bhell\\\\b\")\n  ) \nsum_words_macbeth &lt;-opposite_words_macbeth %&gt;% \n  summarize(\n    love = sum(w_love), \n    hate = sum(w_hate), \n    life = sum(w_life), \n    death = sum(w_death), \n    heaven = sum(w_heaven),\n    hell = sum(w_hell)\n  )\n\n\n\n\nCode\nromeo_juliet_df &lt;-pivot_longer(sum_words_romeo_juliet, cols=everything(), names_to = \"word\", values_to = \"count\")\nhamlet_df &lt;- pivot_longer(sum_words_hamlet, cols=everything(), names_to = \"word\", values_to = \"count\")\nmacbeth_df &lt;- pivot_longer(sum_words_macbeth, cols=everything(), names_to = \"word\", values_to = \"count\")\n\n\n\n\nCode\nromeo_juliet_df &lt;- romeo_juliet_df %&gt;% \n  mutate(word = fct_relevel(word, \"love\", \"hate\", \"life\", \"death\", \"heaven\", \"hell\"))\nhamlet_df &lt;- hamlet_df %&gt;% \n  mutate(word = fct_relevel(word, \"love\", \"hate\", \"life\", \"death\", \"heaven\", \"hell\"))\nmacbeth_df &lt;- macbeth_df %&gt;% \n  mutate(word = fct_relevel(word, \"love\", \"hate\", \"life\", \"death\", \"heaven\", \"hell\"))\n\n\n\n\nCode\nggplot(romeo_juliet_df, aes(x=word, y=count)) + \n  geom_col() +\n  labs(x=\"Word\", \n       y=\"Count\",\n       title=\"Word counts in Romeo and Juliet\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(hamlet_df, aes(x=word, y=count)) + \n  geom_col() +\n  labs(x=\"Word\", \n       y=\"Count\",\n       title=\"Word counts in Hamlet\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(macbeth_df, aes(x=word, y=count)) + \n  geom_col() +\n  labs(x=\"Word\", \n       y=\"Count\",\n       title=\"Word counts in Macbeth\")\n\n\n\n\n\n\n\n\n\nI wanted to expand on the first plot’s theme of opposite word frequency. I identified three word pairs to count using the detect function, and plotted the distribution for all three plays. The highest total for any word is “love” in Romeo and Juliet, which makes sense. The play also favors “death” to “life” much more heavily than the other two plays. Life and death are split equally in Macbeth (and both slightly higher than “love”). “Hate” doesn’t seem to be as common a word for Shakespeare.\nSources:\nData: Massachussetts Institute of Technology, “The Complete Works of William Shakespeare” https://shakespeare.mit.edu/\nShakespeare, William, “Hamlet” https://shakespeare.mit.edu/hamlet/\nShakespeare, William, “Macbeth” https://shakespeare.mit.edu/macbeth/\nShakespeare, William, “Romeo and Juliet” https://shakespeare.mit.edu/romeo_juliet/\nTidyTuesday: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-09-17/readme.md"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Target’s Data Analysis of Pregnancy",
    "section": "",
    "text": "In his 2012 article “How Companies Learn Your Secrets”, NYT writer Charles Duhigg detailed how Target used/uses purchasing data linked to consumer profiles to enhance its marketing techniques. Duhigg referenced a study that found purchases become a rigid habit for consumers, without active decision-making over time. A major life shift, however, can make purchasing habits more flexible in ways that retailers can take advantage of to establish new purchasing habits. In 2002, Target hired statistician Andrew Pole to analyze customers’ purchasing habits. In conversation with Duhigg, Pole explained how he was tasked with developing a model that could predict the likelihood that a customer was pregnant (or preparing to be) based on their purchases. Target assigns each customer a unique ID number linked to their name, credit card, or email address. Each ID holds all the items that person purchased, along with other demographic information Target has gathered (or bought from other sources). Purchasing patterns are collected in accordance with Target’s privacy policy, which indicates that all of a consumer’s “commercial information” may be gathered. This data is collected both in-store and through Target’s website and mobile app.\nPole began gathering data from Target’s baby shower registries, product lists that are publicly available. He tested the data and found that pregnant women tend to buy certain products, such as unscented lotions and soap, specific supplements, and cotton balls. Pole gathered 25 products that could be analyzed together to produce a “pregnancy prediction” score. Additionally, the women using the registry could enter their due date, so Pole was able to match certain products with certain stages of pregnancy. Thus, the purchased/requested products used to generate a pregnancy likelihood could also be used to estimate that woman’s due date. This program was applied to all women who regularly shopped at Target. By tracking those who were likely pregnant, Target could direct advertisements and coupons for baby products, build purchasing habits that would last. From the consumer perspective, there are clear concerns about this marketing practice. Target analyzed women’s purchasing behaviors to make inferences about their personal lives. This analysis was done without the represented individuals’ knowledge to target them for marketing. Regardless of its purpose, using retail data to infer pregnancy and act on those assumptions raises clear ethical concerns. \nConsent Structure\nConsent is a crucial part of protecting the safety and privacy of the people represented by the data. There is no formal consent structure for customers. It is expected that purchasing products at a retailer provides data (what you purchased) linked to an identifier you willingly provide (i.e., credit card or email). The women who were assessed by the pregnancy predictor model did not know that their commercial information would be used in this specific way. Target’s privacy policy declares that this information can be used for “targeted advertising” and research. Beyond this, informed consent for the pregnancy model would not be possible without a direct interaction with each woman who buys something from Target. Thus, giving informed consent to have your purchasing data used in this specific way would be challenging. \nIdentifiable Data\nFor Target’s process, purchasing must be matched with the corresponding customer ID. The goal of analyzing the commercial data is to determine the likelihood of pregnancy and send targeted advertisements. The data is not publicly available, but for Target’s purposes, all of it is identifiable. If a woman buys a certain product, it will be used to predict pregnancy, a result that is matched with her ID indefinitely. \nRespect for Privacy\nAlthough the data is not public, respect for privacy remains questionable. The data itself (what you purchase) is not private, but a customer’s use of purchased products in her personal life should be private. The goal of the predictor model is to make estimates about a woman’s personal life. While the data gathering itself does not infringe on privacy, the model it is used for does so. Sending advertisements based on the outcomes is a key piece of this. Duhigg described one example of a father who was upset his daughter received ads for baby products, only to find out she was pregnant. The ads and coupons represent Target’s assumption/estimate of pregnancy, and regardless of their accuracy, they may have unintended consequences for the woman receiving them. Thus, there are very significant privacy issues with not just Target’s data analysis, but also the resulting targeted marketing strategies. \nUnintended consequences\nThe unintended consequences on privacy are clear ethical implications that are not significant in Target’s marketing strategy. Target collected and used the data without consideration for the individuals. After some pushback, the targeted ads were diluted with other ads unrelated to pregnancy or child care. This ended up boosting the likelihood that the coupons for baby products would be used. If a woman “believes she hasn’t been spied on, she’ll use the coupons,” said Pole. Clearly, the targeted ads were not received well, which highlights the personal and emotional impact on the women whose personal lives were estimated without their knowledge. \nThe shift in advertising represents the clear, profit-oriented goal of Target’s study. The impact was considered only insofar as it affected customer habits. The goal of the analysis was to manipulate these habits. While the data collection does not violate customer privacy, the potential implications of estimating pregnancy were not considered.  \nSources \nDuhigg, Charles. “How Companies Learn Your Secrets.” The New York Times, Feb. 16, 2012, https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html?unlocked_article_code=1.6U4.nLSt.-yehBjKHEmPW&smid=url-share \nTarget. “Target Privacy Policy.” https://www.target.com/c/target-privacy-policy/-/N-4sr7p"
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Traffic & Pedestrian Stop Analysis",
    "section": "",
    "text": "Code\nlibrary(DBI)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(RMariaDB)\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nI investigate trends in traffic/pedestrian stop data from my hometown of San Francisco. The first analyses I did was frequency throughout the day. I sought to visualize the trends over the course of the day by grouping by time and plotting each count. The time for each stop was marked with the hour and minute (seconds not specified) for San Francisco, so I grouped by the time variable to count the frequency at each minute in the day.\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\nbeat\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\n\n\n\n\n\nCode\nSELECT time, COUNT(*) AS num_stops\nFROM ca_san_francisco_2020_04_01\nWHERE time IS NOT NULL \nGROUP BY time \nORDER BY time;\n\n\n\n\nCode\ntime_table %&gt;% \n  ggplot(aes(x = time, y = num_stops)) +\n  geom_point() +\n  labs(x = \"Time\", \n       y = \"Number of Stops\", \n       title = \"Number of stops throughout day in San Francisco\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nSELECT time, COUNT(*) AS num_stops\nFROM ca_san_francisco_2020_04_01\nWHERE time BETWEEN '8:00:00' AND '10:00:00'\nGROUP BY time \nORDER BY time;\n\n\n\n\nCode\ntime_table8to10 %&gt;% \n  ggplot(aes(x = time, \n             y = num_stops, \n             color = minute(time) %in% c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55))) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Time\", \n       y = \"Number of Stops\", \n       title = \"Number of stops from 8AM to 10AM\")\n\n\n\n\n\n\n\n\n\nBased on the plot of counts at each minute, there is an evident discrepancy in the time data. Upon observing the data, it is clear that for minute multiples of 5 (and especially 10) the count spikes (i.e. 1:30 much higher than 1:29 and 1:31). This can be viewed in the second plot (stop frequency from 8AM to 10AM), where times every 5 minute are blue. It follows that when an individual traffic stop is recorded, the time of day is often rounded to the nearest regular 5th minute. To account for this, I grouped data by rounding each time down to the hour. I include data from the three Bay Area data sets (San Francisco, Oakland, and San Jose).\n\n\nCode\nSELECT COUNT(*) AS num_stops, HOUR(time) AS hour, \"San Francisco\" AS city\nFROM ca_san_francisco_2020_04_01\nGROUP BY hour \n\n\nUNION ALL \n\nSELECT COUNT(*) AS num_stops, HOUR(time) AS hour, \"Oakland\" AS city \nFROM ca_oakland_2020_04_01\nGROUP BY hour \n\n\nUNION ALL\n\nSELECT COUNT(*) AS num_stops, HOUR(time) AS hour, \"San Jose\" AS city \nFROM ca_san_jose_2020_04_01\nGROUP BY hour;\n\n\n\n\nCode\nbay_total %&gt;% \n  ggplot(aes(\n    x = hour,\n    y = num_stops, \n    color = city \n  )) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Time of Day\", \n       y = \"Total Stops\", \n       title = \"Total Stops throughout the Day\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSan Francisco has substantially more stops, with greater variation during the day. There is a clear drop in the early morning after 12AM, before the count begins to rise again around 6AM. This is likely when the number of cars on the road starts to increase. The count increases until around 9AM, where the points fluctuate but remain relatively steady until an increase at around 4PM. The count peaks around 5-6PM, and decreases until around 9PM when it rises again. The stop counts in San Jose and Oakland follow similar patterns during the day. They seem to match (similar to San Francisco) commute patterns by rising in the morning from 5 until 9, and rising again in the afternoon.\nMean of hourly stop count\n\n\nCode\nbay_total %&gt;% \n  group_by(city) %&gt;% \n  summarize(mean = mean(num_stops))\n\n\n# A tibble: 3 × 2\n  city             mean\n  &lt;chr&gt;         &lt;int64&gt;\n1 Oakland          5336\n2 San Francisco   36202\n3 San Jose         6368\n\n\n\n\nCode\nSELECT HOUR(time) AS hour, SUM(arrest_made) AS num_arrests, \"San Francisco\" AS city, (SUM(arrest_made)/COUNT(*)) * 100 AS arrest_perc\nFROM ca_san_francisco_2020_04_01\nWHERE time IS NOT NULL \nGROUP BY hour\n\n\nUNION ALL\n\nSELECT HOUR(time) AS hour, SUM(arrest_made) AS num_arrests, \"Oakland\" AS city, (SUM(arrest_made)/COUNT(*)) * 100 AS arrest_perc \nFROM ca_oakland_2020_04_01\nWHERE time IS NOT NULL\nGROUP BY hour\n\n\nUNION ALL\n\nSELECT HOUR(time) as hour, SUM(arrest_made) AS num_arrests, \"San Jose\" AS city, (SUM(arrest_made)/COUNT(*)) * 100 AS arrest_perc \nFROM ca_san_jose_2020_04_01\nWHERE time IS NOT NULL\nGROUP BY hour;\n\n\n\n\nCode\nbay_table %&gt;% \n  ggplot(aes(\n    x = hour,\n    y = arrest_perc, \n    color = city \n  )) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Time of Day\", \n       y = \"% Arrest Made\", \n       title = \"Percent of Stops Ending with Arrest\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot use data from the three Bay Area data sets to show the percent of stops that end in an arrest at each hour during the day. Data points were grouped using the hour() function to round the time down to the nearest hour. Patterns across time vary between the cities. San Francisco has relatively a constant low arrest made% througout the whole day. On the other hand, Oakland has much higher arrest/total stops values throughout the day, peaking at 5 to 7AM. San Jose has lower rates than Oakland for most of the day, but is higher around its peak from 9 to 11 AM. This could reflect different policing patterns and crime patterns in different cities during the day.\nMean of hourly arrest%\n\n\nCode\nbay_table %&gt;% \n  group_by(city) %&gt;% \n  summarize(mean = mean(arrest_perc))\n\n\n# A tibble: 3 × 2\n  city           mean\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Oakland       13.4 \n2 San Francisco  1.36\n3 San Jose       7.69\n\n\nThe low percentage for San Francisco could be because of its much higher traffic stop count. Stricter guidelines for traffic and pedestrian stops may lead to a much higher enforcement of smaller infractions that do not necessarily end in an arrest. Oakland and San Jose may only focus on stopping more serious infractions, potentially due to administrative and resource limitations and greater area to cover.\nAnother potential issue could be reporting, where stops with no reported infraction are not officially recorded. This video reports on Oakland’s low clearance rate (arrests/reported crimes) and problems with data collection. The low clearance rate for reported crimes contrasts with the high rate of arrests made per stop from my analysis, but traffic stop arrests and reported crime arrests are different. These issues may lead to limited effort on minor traffic/pedestrian infractions, causing stops ending in arrests to take up a larger proportion.\nReferences\nE. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, D. Jenson, A. Shoemaker, V. Ramachandran, P. Barghouty, C. Phillips, R. Shroff, and S. Goel. “A large-scale analysis of racial disparities in police stops across the United States”. Nature Human Behaviour, Vol. 4, 2020.\nABC7 News Bay Area, “Oakland PD’s inaccurate crime reporting leads to questions about what’s going on in city”. Youtube, 2024. https://www.youtube.com/watch?v=mPLxsv4IrUw"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xander Nonaka",
    "section": "",
    "text": "Hi! My name is Xander, I’m a senior at Pomona College from San Francisco, CA. I spent most of my childhood in SF, but also lived in Tokyo, Japan for five years. The photo here is one I took of the redwood trees at Muir Woods, north of San Francisco. Exploring California’s natural features sparked my interest in the environment, which motivated me to study environmental policy at Pomona. I’m also passionate about sports and music, and spend my time watching Bay Area sports and taking music lessons."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "traffic.html",
    "href": "traffic.html",
    "title": "Daily England Traffic",
    "section": "",
    "text": "England National Highways Traffic Data Set\nThe data set used in this plot contains speed and vehicle volume data captured throughout the day over the course of many days on different highway sites in England.\n\n\nCode\ntraffic &lt;- A64_traffic %&gt;% \n  group_by(`Time Interval`) %&gt;% \n  summarize(avg_speed = mean(`Avg mph`, na.rm=TRUE)) %&gt;% \n  arrange(desc(avg_speed))\n\nggplot(traffic, \n       aes(\n         x=`Time Interval`,\n         y=avg_speed,\n       )) +\n  geom_line() +\n  labs(x=\"Time of Day\",\n       y=\"Average Speed (mph)\",\n       title=\"Highway Traffic Speed in England throughout the day\")\n\n\n\n\n\n\n\n\n\nAverage speed across all highway sites is measured relative to the time of day the data was captured. Time is displayed here as a series of interval numbers from 0 to 95. 0 represents the first period of data on a new day, and 95 represents the last. Thus noon is around the 47th interval.\nSources:\nNational Highways, “WebTRIS Traffic Flow API.” WebTRIS. https://webtris.nationalhighways.co.uk/api/swagger/ui/index\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-12-03/readme.md"
  }
]